{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "606fe292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marzy/Documents/qa-task/code/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9008/9008 [00:03<00:00, 2401.16 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 930/930 [00:00<00:00, 2798.38 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2252/2252 [00:05<00:00, 390.77 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 465/465 [00:00<00:00, 534.98 examples/s]\n",
      "/home/marzy/Documents/qa-task/code/venv/lib64/python3.11/site-packages/transformers/quantizers/auto.py:231: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: unsloth/Llama-3.2-1B-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =======================================================\n",
    "# Task A - Persian Generative QA with QLoRA (Llama-3.2-1B-bnb-4bit)\n",
    "# =======================================================\n",
    "\n",
    "import os, re, json, random, torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from huggingface_hub import login\n",
    "from tqdm import tqdm\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "OUTPUT_DIR = \"./qadata\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "TRAIN_FRACTION = 0.25  # For quick testing; set to 1.0 for full\n",
    "VAL_FRACTION   = 0.50\n",
    "TRAIN_MAX_SAMPLES = None\n",
    "VAL_MAX_SAMPLES = None\n",
    "MODEL_ID = \"unsloth/Llama-3.2-1B-bnb-4bit\"\n",
    "BATCH_SIZE = 8\n",
    "GR_ACCUM   = 2\n",
    "EPOCHS     = 1  # Increase to 2-3 for better results\n",
    "LR         = 2e-4\n",
    "MAX_LENGTH = 1024\n",
    "DOC_STRIDE = 128\n",
    "WARMUP_RATIO = 0.03\n",
    "# Login if needed (for gated models)\n",
    "# token = 'your_hf_token'\n",
    "# login(token)\n",
    "# -------------------------\n",
    "# Data loader\n",
    "# -------------------------\n",
    "def read_qa(path):\n",
    "    ds = []\n",
    "    with open(Path(path), encoding=\"utf-8\") as f:\n",
    "        squad = json.load(f)\n",
    "    for example in squad[\"data\"]:\n",
    "        title = example.get(\"title\", \"\").strip()\n",
    "        for paragraph in example[\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"].strip()\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                answers = [a[\"text\"].strip() for a in qa[\"answers\"]]\n",
    "                answer_starts = [a[\"answer_start\"] for a in qa[\"answers\"]]\n",
    "                ds.append({\n",
    "                    \"title\": title,\n",
    "                    \"context\": context,\n",
    "                    \"question\": qa[\"question\"].strip(),\n",
    "                    \"id\": qa[\"id\"],\n",
    "                    \"answers\": {\"answer_start\": answer_starts, \"text\": answers}\n",
    "                })\n",
    "    return ds\n",
    "train_ds = read_qa(\"./qadata/pqa_train.json\")\n",
    "val_ds   = read_qa(\"./qadata/pqa_test.json\")\n",
    "train_dataset = Dataset.from_list(train_ds)\n",
    "val_dataset   = Dataset.from_list(val_ds)\n",
    "raw_ds = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n",
    "# -------------------------\n",
    "# Persian normalization\n",
    "# -------------------------\n",
    "def normalize_persian(text: str) -> str:\n",
    "    if not text: return \"\"\n",
    "    text = text.replace(\"\\u200c\", \" \").replace(\"ÙŠ\",\"ÛŒ\").replace(\"Ùƒ\",\"Ú©\")\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "def map_to_squad(example):\n",
    "    answers = {\"text\": [normalize_persian(t) for t in example[\"answers\"][\"text\"]],\n",
    "               \"answer_start\": example[\"answers\"][\"answer_start\"]}\n",
    "    return {\n",
    "        \"id\": str(example.get(\"id\", \"\")),\n",
    "        \"context\": normalize_persian(example[\"context\"]),\n",
    "        \"question\": normalize_persian(example[\"question\"]),\n",
    "        \"answers\": answers,\n",
    "    }\n",
    "mapped = raw_ds.map(map_to_squad)\n",
    "# -------------------------\n",
    "# Subset for speed\n",
    "# -------------------------\n",
    "def take_subset(ds_split, frac=None, max_samples=None, seed=SEED):\n",
    "    idxs = list(range(len(ds_split)))\n",
    "    random.Random(seed).shuffle(idxs)\n",
    "    if frac: idxs = idxs[:max(1, int(len(ds_split)*frac))]\n",
    "    if max_samples: idxs = idxs[:max_samples]\n",
    "    return ds_split.select(idxs)\n",
    "train_small = take_subset(mapped[\"train\"], frac=TRAIN_FRACTION, max_samples=TRAIN_MAX_SAMPLES)\n",
    "val_small   = take_subset(mapped[\"validation\"], frac=VAL_FRACTION, max_samples=VAL_MAX_SAMPLES)\n",
    "# -------------------------\n",
    "# Tokenizer\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "# -------------------------\n",
    "# Prepare features for generative QA\n",
    "# -------------------------\n",
    "def prepare_qa_features(examples):\n",
    "    prompts = []\n",
    "    for q, c, ans in zip(examples[\"question\"], examples[\"context\"], examples[\"answers\"]):\n",
    "        answer_text = ans[\"text\"][0] if ans[\"text\"] else \"\"\n",
    "        prompt = f\"Ø²Ù…ÛŒÙ†Ù‡: {c}\\nØ³ÙˆØ§Ù„: {q}\\nÙ¾Ø§Ø³Ø®: {answer_text}\"\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Create labels for LM (shifted input_ids, -100 for non-answer parts)\n",
    "    labels = tokenized.input_ids.clone()\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    # Optionally, mask loss to only answer part (after \"Ù¾Ø§Ø³Ø®:\")\n",
    "    for i in range(len(prompts)):\n",
    "        prompt_len = len(tokenizer(f\"Ø²Ù…ÛŒÙ†Ù‡: {examples['context'][i]}\\nØ³ÙˆØ§Ù„: {examples['question'][i]}\\nÙ¾Ø§Ø³Ø®:\")[\"input_ids\"])\n",
    "        labels[i, :prompt_len] = -100\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "train_features = train_small.map(prepare_qa_features, batched=True, remove_columns=train_small.column_names)\n",
    "val_features   = val_small.map(prepare_qa_features, batched=True, remove_columns=val_small.column_names)\n",
    "# -------------------------\n",
    "# QLoRA config & model\n",
    "# -------------------------\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0]>=8 else torch.float16\n",
    ")\n",
    "lora_config = LoraConfig(\n",
    "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, quantization_config=bnb_config, device_map=\"cpu\", trust_remote_code=True\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"Using model:\", MODEL_ID)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26889aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marzy/Documents/qa-task/code/venv/lib64/python3.11/site-packages/transformers/training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_103841/1378625441.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# -------------------------\n",
    "# Trainer & Training\n",
    "# -------------------------\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0]>=8\n",
    "args=TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GR_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    remove_unused_columns=False,\n",
    "    bf16=False,\n",
    "    fp16=False,\n",
    "    report_to=[],\n",
    "    no_cuda=True,  # <--- Add this line\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "trainer = Trainer(\n",
    "    model=model, args=args, train_dataset=train_features, eval_dataset=val_features,\n",
    "    tokenizer=tokenizer, data_collator=data_collator\n",
    ")\n",
    "trainer.train()\n",
    "# Save adapters\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"lora_adapters\"))\n",
    "# -------------------------\n",
    "# Evaluation: F1 & EM\n",
    "# -------------------------\n",
    "def normalize_for_eval(s: str) -> str:\n",
    "    s = normalize_persian(s)\n",
    "    s = re.sub(r\"[\\p{P}ØŒØ›ØŸ]+\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "def f1_score(prediction: str, ground_truth: str) -> float:\n",
    "    pred_tokens = normalize_for_eval(prediction).split()\n",
    "    gt_tokens = normalize_for_eval(ground_truth).split()\n",
    "    common = {t: min(pred_tokens.count(t), gt_tokens.count(t)) for t in set(pred_tokens)}\n",
    "    num_same = sum(common.values())\n",
    "    if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
    "        return float(pred_tokens == gt_tokens)\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(gt_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "def exact_match_score(prediction: str, ground_truth: str) -> float:\n",
    "    return float(normalize_for_eval(prediction) == normalize_for_eval(ground_truth))\n",
    "model.eval()\n",
    "preds = []\n",
    "refs = []\n",
    "for example in tqdm(val_small):\n",
    "    q = example[\"question\"]\n",
    "    c = example[\"context\"]\n",
    "    gold = example[\"answers\"][\"text\"][0] if example[\"answers\"][\"text\"] else \"\"\n",
    "    refs.append(gold)\n",
    "    \n",
    "    prompt = f\"Ø²Ù…ÛŒÙ†Ù‡: {c}\\nØ³ÙˆØ§Ù„: {q}\\nÙ¾Ø§Ø³Ø®:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, max_new_tokens=50, num_beams=1, do_sample=False)\n",
    "    generated = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Ù¾Ø§Ø³Ø®:\")[-1].strip()\n",
    "    preds.append(generated)\n",
    "ems = [exact_match_score(p, r) for p, r in zip(preds, refs)]\n",
    "f1s = [f1_score(p, r) for p, r in zip(preds, refs)]\n",
    "EM = sum(ems) / len(ems) if ems else 0.0\n",
    "F1 = sum(f1s) / len(f1s) if f1s else 0.0\n",
    "print({\"Exact Match\": EM, \"F1\": F1})\n",
    "# Save metrics\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"exact_match\": EM, \"f1\": F1}, f, ensure_ascii=False, indent=2)\n",
    "print(\"Done. Metrics saved at:\", os.path.join(OUTPUT_DIR, \"metrics.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
